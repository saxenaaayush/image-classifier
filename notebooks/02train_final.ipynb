{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T04:26:59.107012Z",
     "iopub.status.busy": "2025-07-13T04:26:59.106562Z",
     "iopub.status.idle": "2025-07-13T04:27:05.741917Z",
     "shell.execute_reply": "2025-07-13T04:27:05.741307Z",
     "shell.execute_reply.started": "2025-07-13T04:26:59.106987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split,Subset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR,CosineAnnealingLR,SequentialLR,LinearLR\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from torchvision import models\n",
    "from torchvision.models import (\n",
    "    EfficientNet_B0_Weights,\n",
    "    EfficientNet_B1_Weights,\n",
    "    EfficientNet_B2_Weights,\n",
    "    EfficientNet_B3_Weights,\n",
    "    EfficientNet_B4_Weights)\n",
    "from torch.nn.functional import sigmoid\n",
    "import cv2\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:45:37.637308Z",
     "iopub.status.busy": "2025-07-13T08:45:37.637037Z",
     "iopub.status.idle": "2025-07-13T08:45:37.643100Z",
     "shell.execute_reply": "2025-07-13T08:45:37.642384Z",
     "shell.execute_reply.started": "2025-07-13T08:45:37.637287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T04:27:26.419923Z",
     "iopub.status.busy": "2025-07-13T04:27:26.419326Z",
     "iopub.status.idle": "2025-07-13T04:27:26.425375Z",
     "shell.execute_reply": "2025-07-13T04:27:26.424648Z",
     "shell.execute_reply.started": "2025-07-13T04:27:26.419898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_std(root_dir):\n",
    "    exts = (\"*.png\",)\n",
    "    sum_   = np.zeros(3, dtype=np.float64)\n",
    "    sum_sq = np.zeros(3, dtype=np.float64)\n",
    "    cnt    = 0\n",
    "\n",
    "    files = []\n",
    "    for split in (\"train\", \"test\"):\n",
    "        for cls in (\"normal\",\"cataract\"):\n",
    "            for ext in exts:\n",
    "                files += glob(os.path.join(root_dir, \"processed_images\", split, cls, ext))\n",
    "\n",
    "    for f in tqdm(files, desc=\"Computing mean/std\"):\n",
    "        img = np.array(Image.open(f).convert(\"RGB\")) / 255.0\n",
    "        sum_   += img.mean(axis=(0,1))\n",
    "        sum_sq += (img**2).mean(axis=(0,1))\n",
    "        cnt    += 1\n",
    "\n",
    "    mean = sum_ / cnt\n",
    "    var  = (sum_sq / cnt) - mean**2\n",
    "    std  = np.sqrt(var)\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:45:33.971899Z",
     "iopub.status.busy": "2025-07-13T08:45:33.971608Z",
     "iopub.status.idle": "2025-07-13T08:45:33.975836Z",
     "shell.execute_reply": "2025-07-13T08:45:33.975282Z",
     "shell.execute_reply.started": "2025-07-13T08:45:33.971878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_MEAN, _STD=[0.6257231324993875, 0.4934742948338769, 0.42569583700621416],[0.25667137400692847, 0.2345312511218496, 0.2305881956020596]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:50:59.689663Z",
     "iopub.status.busy": "2025-07-13T08:50:59.689414Z",
     "iopub.status.idle": "2025-07-13T08:50:59.695352Z",
     "shell.execute_reply": "2025-07-13T08:50:59.694644Z",
     "shell.execute_reply.started": "2025-07-13T08:50:59.689646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dir, split=\"train\", transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        split_dir = os.path.join(dir, \"processed_images\", split)\n",
    "        for label_name, label_idx in [(\"normal\", 0), (\"cataract\", 1)]:\n",
    "            for img_path in glob(os.path.join(split_dir, label_name, \"*.png\")):\n",
    "                self.samples.append((img_path, label_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:51:01.973417Z",
     "iopub.status.busy": "2025-07-13T08:51:01.972696Z",
     "iopub.status.idle": "2025-07-13T08:51:01.978966Z",
     "shell.execute_reply": "2025-07-13T08:51:01.978315Z",
     "shell.execute_reply.started": "2025-07-13T08:51:01.973393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    root,\n",
    "    batch_size=32,\n",
    "    val_frac=0.2,\n",
    "    num_workers=2,\n",
    "    train_transform=None,\n",
    "    val_transform=None\n",
    "):\n",
    "\n",
    "    raw_ds = ImageDataset(root, \"train\", transform=None)\n",
    "    n = len(raw_ds)\n",
    "    val_size = int(n * val_frac)\n",
    "    train_size = n - val_size\n",
    "\n",
    "    all_indices = list(range(n))\n",
    "    train_indices, val_indices = random_split(all_indices, [train_size, val_size])\n",
    "\n",
    "    train_ds = Subset(\n",
    "        ImageDataset(root, \"train\", transform=train_transform),\n",
    "        train_indices\n",
    "    )\n",
    "    val_ds = Subset(\n",
    "        ImageDataset(root, \"train\", transform=val_transform),\n",
    "        val_indices\n",
    "    )\n",
    "    test_ds = ImageDataset(root, \"test\", transform=val_transform)\n",
    "\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True),\n",
    "        DataLoader(val_ds,   batch_size, shuffle=False, num_workers=num_workers, pin_memory=True),\n",
    "        DataLoader(test_ds,  batch_size, shuffle=False, num_workers=num_workers, pin_memory=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:44:51.166155Z",
     "iopub.status.busy": "2025-07-13T08:44:51.165659Z",
     "iopub.status.idle": "2025-07-13T08:44:51.172120Z",
     "shell.execute_reply": "2025-07-13T08:44:51.171559Z",
     "shell.execute_reply.started": "2025-07-13T08:44:51.166131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(backbone=\"resnet50\", pretrained=True):\n",
    "    if backbone.startswith(\"resnet\"):\n",
    "        model = getattr(models, backbone)(pretrained=pretrained)\n",
    "        in_feats = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(in_feats, 1)\n",
    "        \n",
    "        )\n",
    "\n",
    "    elif backbone.startswith(\"efficientnet\"):\n",
    "        if pretrained:\n",
    "            weights_map = {\n",
    "                \"efficientnet_b0\": EfficientNet_B0_Weights.DEFAULT,\n",
    "                \"efficientnet_b1\": EfficientNet_B1_Weights.DEFAULT,\n",
    "                \"efficientnet_b2\": EfficientNet_B2_Weights.DEFAULT,\n",
    "                \"efficientnet_b3\": EfficientNet_B3_Weights.DEFAULT,\n",
    "                \"efficientnet_b4\": EfficientNet_B4_Weights.DEFAULT\n",
    "            }\n",
    "            weights = weights_map.get(backbone, None)\n",
    "            if weights is None:\n",
    "                raise ValueError(f\"Pretrained weights not available for {backbone}\")\n",
    "            model = getattr(models, backbone)(weights=weights)\n",
    "        else:\n",
    "            model = getattr(models, backbone)(weights=None)\n",
    "\n",
    "        in_feats = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_feats, 1)\n",
    "        \n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:44:53.289364Z",
     "iopub.status.busy": "2025-07-13T08:44:53.288650Z",
     "iopub.status.idle": "2025-07-13T08:44:53.295208Z",
     "shell.execute_reply": "2025-07-13T08:44:53.294610Z",
     "shell.execute_reply.started": "2025-07-13T08:44:53.289338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_single_epoch(model, train_loader, optimizer, scheduler, loss_fn, device):\n",
    "    model.train()\n",
    "    t_loss, preds, lbl = 0, [], []\n",
    "\n",
    "    for imgs, labs in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        imgs, labs = imgs.to(device), labs.to(device).unsqueeze(1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = loss_fn(logits, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        probs = torch.sigmoid(logits.detach().cpu())\n",
    "        labs_cpu = labs.detach().cpu()\n",
    "\n",
    "        if probs.ndim > 1:\n",
    "            probs = probs.squeeze(1)\n",
    "        if labs_cpu.ndim > 1:\n",
    "            labs_cpu = labs_cpu.squeeze(1)\n",
    "\n",
    "        preds += probs.tolist()\n",
    "        lbl += labs_cpu.tolist()\n",
    "        t_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    avg_loss = t_loss / len(train_loader.dataset)\n",
    "    auc = roc_auc_score(lbl, preds)\n",
    "    acc = accuracy_score(lbl, [p > 0.5 for p in preds])\n",
    "    return avg_loss, auc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:44:56.107726Z",
     "iopub.status.busy": "2025-07-13T08:44:56.107167Z",
     "iopub.status.idle": "2025-07-13T08:44:56.113524Z",
     "shell.execute_reply": "2025-07-13T08:44:56.112809Z",
     "shell.execute_reply.started": "2025-07-13T08:44:56.107703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def eval_one_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    preds, lbl = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labs in loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device).unsqueeze(1).float()\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labs)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits.detach().cpu())\n",
    "            labs_cpu = labs.detach().cpu()\n",
    "            if probs.ndim > 1:   probs = probs.squeeze(1)\n",
    "            if labs_cpu.ndim > 1: labs_cpu = labs_cpu.squeeze(1)\n",
    "\n",
    "            preds += probs.tolist()\n",
    "            lbl   += labs_cpu.tolist()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    auc      = roc_auc_score(lbl, preds)\n",
    "    acc      = accuracy_score(lbl, [p>0.5 for p in preds])\n",
    "    return avg_loss, auc, acc, preds, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_outputs(y_true, y_pred_logits, threshold=0.5):\n",
    "    \n",
    "    probs = torch.sigmoid(torch.tensor(y_pred_logits)).numpy()\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    y_true = np.array(y_true)\n",
    "\n",
    "    auc = roc_auc_score(y_true, probs)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    report = classification_report(y_true, preds, target_names=[\"Normal\", \"Cataract\"])\n",
    "    cm = confusion_matrix(y_true, preds)\n",
    "\n",
    "    print(\"\\n[Classification Report]\\n\")\n",
    "    print(report)\n",
    "    print(f\"ROC AUC Score: {auc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Cataract\"], yticklabels=[\"Normal\", \"Cataract\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"classification_report\": report,\n",
    "        \"roc_auc\": auc,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:44:58.188596Z",
     "iopub.status.busy": "2025-07-13T08:44:58.188328Z",
     "iopub.status.idle": "2025-07-13T08:44:58.194049Z",
     "shell.execute_reply": "2025-07-13T08:44:58.193481Z",
     "shell.execute_reply.started": "2025-07-13T08:44:58.188574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clean_transforms_iter1 = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "])\n",
    "\n",
    "train_transforms_iter1 = transforms.Compose([\n",
    "    transforms.Resize(400),\n",
    "    transforms.CenterCrop(380),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1, 0.1),\n",
    "    transforms.RandomAdjustSharpness(0.7),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_MEAN, _STD)\n",
    "])\n",
    "\n",
    "val_transforms_iter1 = transforms.Compose([\n",
    "    transforms.Resize(400),\n",
    "    transforms.CenterCrop(380),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_MEAN, _STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:50:52.136844Z",
     "iopub.status.busy": "2025-07-13T08:50:52.136469Z",
     "iopub.status.idle": "2025-07-13T08:50:52.141851Z",
     "shell.execute_reply": "2025-07-13T08:50:52.141050Z",
     "shell.execute_reply.started": "2025-07-13T08:50:52.136819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"data/raw/cataract-image-dataset/processed_images\"\n",
    "BATCH_SIZE   = 32\n",
    "VAL_FRAC     = 0.2\n",
    "\n",
    "BACKBONE     = \"efficientnet_b4\"     # \"resnet_50\"\n",
    "PRETRAINED   = True\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS       = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:51:06.081711Z",
     "iopub.status.busy": "2025-07-13T08:51:06.081459Z",
     "iopub.status.idle": "2025-07-13T08:51:06.099945Z",
     "shell.execute_reply": "2025-07-13T08:51:06.099250Z",
     "shell.execute_reply.started": "2025-07-13T08:51:06.081693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491 ---- <torch.utils.data.dataset.Subset object at 0x792017d88b10> ... <torch.utils.data.dataset.Subset object at 0x792017d88590>\n"
     ]
    }
   ],
   "source": [
    "train_loader_iter1, val_loader_iter1, test_loader_iter1 = get_dataloaders(\n",
    "    root=DATA_ROOT,\n",
    "    batch_size=8,\n",
    "    val_frac=0.2,\n",
    "    num_workers=2,\n",
    "    train_transform=train_transforms_iter1,\n",
    "    val_transform=val_transforms_iter1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:51:12.176790Z",
     "iopub.status.busy": "2025-07-13T08:51:12.176275Z",
     "iopub.status.idle": "2025-07-13T08:51:12.609628Z",
     "shell.execute_reply": "2025-07-13T08:51:12.609088Z",
     "shell.execute_reply.started": "2025-07-13T08:51:12.176752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_iter1 = build_model(backbone=\"efficientnet_b4\", pretrained=True).to(DEVICE)\n",
    "\n",
    "for param in model_iter1.features[:3].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:51:16.524431Z",
     "iopub.status.busy": "2025-07-13T08:51:16.523867Z",
     "iopub.status.idle": "2025-07-13T08:51:16.530539Z",
     "shell.execute_reply": "2025-07-13T08:51:16.529804Z",
     "shell.execute_reply.started": "2025-07-13T08:51:16.524394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bce_loss_iter1=nn.BCEWithLogitsLoss()\n",
    "\n",
    "opt_iter1 = AdamW(model_iter1.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "warmup_epochs_iter1 = 2\n",
    "\n",
    "scheduler_iter1 = SequentialLR(\n",
    "    opt_iter1,\n",
    "    schedulers=[\n",
    "        LinearLR(opt_iter1, start_factor=0.1, total_iters=warmup_epochs_iter1),\n",
    "        CosineAnnealingLR(opt_iter1, T_max=EPOCHS - warmup_epochs_iter1)\n",
    "    ],\n",
    "    milestones=[warmup_epochs_iter1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:52:28.137281Z",
     "iopub.status.busy": "2025-07-13T08:52:28.136988Z",
     "iopub.status.idle": "2025-07-13T08:55:10.926569Z",
     "shell.execute_reply": "2025-07-13T08:55:10.925836Z",
     "shell.execute_reply.started": "2025-07-13T08:52:28.137255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Train L:0.2141 AUC:0.9809 Acc:0.9338 | Val   L:0.1517 AUC:0.9899 Acc:0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10  Train L:0.1591 AUC:0.9882 Acc:0.9491 | Val   L:0.1273 AUC:0.9912 Acc:0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10  Train L:0.1237 AUC:0.9933 Acc:0.9542 | Val   L:0.1540 AUC:0.9916 Acc:0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10  Train L:0.1197 AUC:0.9921 Acc:0.9618 | Val   L:0.1638 AUC:0.9907 Acc:0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10  Train L:0.1063 AUC:0.9940 Acc:0.9593 | Val   L:0.1084 AUC:0.9920 Acc:0.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10  Train L:0.0960 AUC:0.9939 Acc:0.9771 | Val   L:0.1099 AUC:0.9945 Acc:0.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10  Train L:0.0634 AUC:0.9981 Acc:0.9822 | Val   L:0.1405 AUC:0.9937 Acc:0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10  Train L:0.0621 AUC:0.9985 Acc:0.9771 | Val   L:0.1561 AUC:0.9937 Acc:0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10  Train L:0.0641 AUC:0.9980 Acc:0.9771 | Val   L:0.1408 AUC:0.9941 Acc:0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10  Train L:0.0427 AUC:0.9987 Acc:0.9898 | Val   L:0.1487 AUC:0.9941 Acc:0.9490\n"
     ]
    }
   ],
   "source": [
    "best_val_auc_iter1 = 0.0\n",
    "history_iter1 = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_auc': [],\n",
    "    'val_auc': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    train_loss, train_auc, train_acc = train_single_epoch(\n",
    "        model_iter1, train_loader_iter1,\n",
    "        opt_iter1, scheduler_iter1,\n",
    "        bce_loss_iter1, DEVICE\n",
    "    )\n",
    "\n",
    " \n",
    "    val_loss, val_auc, val_acc, val_preds, val_labels = eval_one_epoch(\n",
    "        model_iter1, val_loader_iter1,\n",
    "        bce_loss_iter1, DEVICE\n",
    "    )\n",
    "\n",
    "\n",
    "    history_iter1['train_loss'].append(train_loss)\n",
    "    history_iter1['val_loss'].append(val_loss)\n",
    "    history_iter1['train_auc'].append(train_auc)\n",
    "    history_iter1['val_auc'].append(val_auc)\n",
    "    history_iter1['train_acc'].append(train_acc)\n",
    "    history_iter1['val_acc'].append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}  \"\n",
    "          f\"Train L:{train_loss:.4f} AUC:{train_auc:.4f} Acc:{train_acc:.4f} | \"\n",
    "          f\"Val   L:{val_loss:.4f} AUC:{val_auc:.4f} Acc:{val_acc:.4f}\")\n",
    "\n",
    "\n",
    "    if val_auc > best_val_auc_iter1:\n",
    "        best_val_auc_iter1 = val_auc\n",
    "        torch.save(model_iter1.state_dict(), \"best_model_effnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model_effnet.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "test_loss, test_auc, test_acc, test_preds, test_labels = eval_one_epoch(\n",
    "    model, test_loader, loss_fn, DEVICE\n",
    ")\n",
    "\n",
    "evaluate_predictions(test_labels, test_preds)\n",
    "\n",
    "print(f\"\\n Test â€” Loss: {test_loss:.4f} | AUC: {test_auc:.4f} | Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3115000,
     "sourceId": 5392218,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
